<!doctype html><html lang=en data-mode=dark><head prefix="og: http://ogp.me/ns#"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.104.0"><meta name=theme content="Color Your World -- gitlab.com/rmaguiar/hugo-theme-color-your-world"><title>An ordinal classification loss - part 1</title><meta name=author content="Alexandre Candeias"><meta name=description content="An ordinal classification loss - part 1"><meta name=robots content="index follow"><link rel=canonical href=https://candeiasalexandre.github.io/posts/an-ordinal-classification-loss-part_1/><meta property="og:site_name" content><meta property="og:title" content="An ordinal classification loss - part 1"><meta property="og:description" content="An ordinal classification loss - part 1"><meta property="og:url" content="https://candeiasalexandre.github.io/posts/an-ordinal-classification-loss-part_1/"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-05-13"><meta property="article:modified_time" content="2023-05-13"><meta property="og:updated_time" content="2023-05-13"><meta name=twitter:creator content="@@alexmricandeias"><meta name=twitter:site content="@@alexmricandeias"><meta name=theme-color content="#222"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="default"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebSite","@id":"https://candeiasalexandre.github.io/"},"headline":"An ordinal classification loss - part 1","description":"An ordinal classification loss - part 1","url":"https://candeiasalexandre.github.io/posts/an-ordinal-classification-loss-part_1/","inLanguage":"en","datePublished":"2023-05-13","dateModified":"2023-05-13","wordCount":"916","publisher":{"@type":"Person","name":"Alexandre Candeias"},"author":{"@type":"Person","name":"Alexandre Candeias","sameAs":["https://github.com/\u003cusername\u003ecandeiasalexandre","https://gitlab.com/\u003cusername\u003ecandeiasalexandre","https://www.linkedin.com/in/\u003cusername\u003ecandeiasalexandre","https://twitter.com/\u003cusername\u003e@alexmricandeias"]}}</script><link rel=stylesheet href=https://candeiasalexandre.github.io/css/main.min.0fdfc4083ce74c4220dde9973823fb6f232399c52d267396c7039677d981e066.css integrity="sha256-D9/ECDznTEIg3emXOCP7byMjmcUtJnOWxwOWd9mB4GY=" crossorigin=anonymous><noscript><meta name=theme-color content="#1dbc91" media="(prefers-color-scheme: dark)"><meta name=theme-color content="#1dbc91" media="(prefers-color-scheme: light)"><link rel=stylesheet href=https://candeiasalexandre.github.io/css/noscript.min.3f3b95436b19eaeb9223fb12c0b86737d53ee0a47fdba271a886c244bc03975c.css integrity="sha256-PzuVQ2sZ6uuSI/sSwLhnN9U+4KR/26JxqIbCRLwDl1w=" crossorigin=anonymous></noscript><link rel=preload href=/fonts/OpenSans-Bold.ttf as=font crossorigin=anonymous><link rel=preload href=/fonts/OpenSans-Italic.ttf as=font crossorigin=anonymous><link rel=preload href=/fonts/OpenSans-Regular.ttf as=font crossorigin=anonymous><link rel=preload href=/fonts/Oswald-Bold.ttf as=font crossorigin=anonymous><link rel=preload href=/libs/katex@0.16.0/dist/fonts/KaTeX_Main-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/libs/katex@0.16.0/dist/fonts/KaTeX_Math-Italic.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/libs/katex@0.16.0/dist/fonts/KaTeX_Size2-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/libs/katex@0.16.0/dist/fonts/KaTeX_Size4-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=me href=https://github.com/%3cusername%3ecandeiasalexandre><link rel=me href=https://gitlab.com/%3cusername%3ecandeiasalexandre><link rel=me href=https://www.linkedin.com/in/%3cusername%3ecandeiasalexandre><link rel=me href=https://twitter.com/%3cusername%3e@alexmricandeias><script src=https://candeiasalexandre.github.io/js/main.7f81509e06270aca2bcac37d12fc87e942ba3be23215aa23d4162b2217f3248a.js integrity="sha256-f4FQngYnCsorysN9EvyH6UK6O+IyFaoj1BYrIhfzJIo=" crossorigin=anonymous></script></head><body><header><a href=/></a><nav aria-label="Main menu."><ul><li><a class=btn href=/>Home</a></li><li><a class=btn href=/posts/>Posts</a></li><li><a class=btn href="https://docs.google.com/document/d/1qNj6_wLQJnJtVbOBkM99N94Z2yn7Z4RENIwnd247NXA/edit?usp=sharing">CV</a></li></ul></nav></header><div class=filler><main><article><header><h1>An ordinal classification loss - part 1</h1><p>Published on <time datetime=2023-05-13>2023-05-13</time></p></header><p>Many times, we find ML problems that can be approached as classification but, after digging into it, we notice that there are hierarchical or ordering relationships between our classes.</p><p>For example, imagine that you are doing review rating prediction. Given an user review you want to give it a score from 1-5. An approach, might be to see the problem as a text classification where there are 5 classes, one for each of the possible ratings.</p><p>For the sake of simplicity imagine that your text is simply represented by an embedding vector, $x \in R^d$. One can use a simple model such a linear classifier given by the function $F(x) = softmax(Wx + b)$, where the model parameters are the matrix $W \in R^{5 \times d}$ and the vector $b \in R^5$.</p><p>This model is predicting a probability distribution $p(\hat{y}|x$) across every possible rating $[1, 2,3,4,5]$.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SimpleLinearClassifier</span>(torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_dim: int, num_class: int) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_linear <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Linear(input_dim, num_class)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x: torch<span style=color:#f92672>.</span>Tensor) <span style=color:#f92672>-&gt;</span> ClassificationOutput:
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_linear(x)
</span></span><span style=display:flex><span>        prob <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>softmax(logits)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> ClassificationOutput(logits, prob)
</span></span></code></pre></div><p>(<em>A simple linear classifier in pytorch.</em>)</p><p>These models are usually trained using the <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>cross entropy loss</a>. In our case for a dataset (or a batch) of size $N$ it is given by:</p><p>$$
L[(x_{1},y_{1}), &mldr; , (x_{N}, y_{N})] =
\frac{\sum_{n=1}^{N} \space l_{n}}{N} \\
= \frac{\sum_{n=1}^{N} CE(p(y_n), p(\hat{y_n}|x_n))}{N} \\
= \frac{
\sum_{n=1}^{N}
\sum_{k=1}^{5} -p_{y_n}(y=k)
\space log(F_k(x))
}{N} \\
= \frac{\sum_{n=1}^{N} \space - log(F_{y_{n}}(x))}{N}
$$</p><p>where $ F_{y_{n}}(x) $ represents the $y_{n}$ entry of the output probability, also bear in mind that $p_{y_n}(y=k)$ is 0 everywhere and 1 when $y=y_n$.</p><p>Now that we know the formula of the loss lets see some examples. Imagine that the output of your model is something as follows:</p><p>$$
F[x_1] = [0.1, 0.2, 0.3, 0.4, 0.5] \\
F[x_2] = [0.5, 0.4, 0.3, 0.2,0.1] \\
F[x_3] = [0.1, 0.5, 0.4, 0.3, 0.2]
$$</p><p>This means that for $x_1$ the model puts more probability in the review score 5, for $x_2$ in 1 and for $x_3$ in the review score 2.
If we assume that the label for all the examples should be the review score 1 and we compute the loss for each of the examples we get the following values:</p><p>$$
y_1=1, l_1=1.8194 \\
y_2=1, l_2=1.4194 \\
y_3=1, l_3=1.8194
$$</p><p>Looking at this values we arrive at a funny conclusion, even though in our problem it makes sense that we would have a bigger loss in the example 1 compared with the example 3, it doesn’t happen. The cross entropy loss doesn’t have into account the ordering in our classes, i.e 1 &lt; 2 &lt; 3 &lt; 4 &lt; 5.</p><p>The question that stands now is: “<strong>How can we make our loss take into consideration the natural order of the review ratings ?”</strong></p><p>After some “googling” we came across <a href=https://github.com/JHart96/keras_ordinal_categorical_crossentropy>this</a> implementation of a regularisation to make the CE loss aware of the ordering of the classes, in summary it can be described as the following equation:</p><p>$$
l_{oce} = CE(p(y), p(\hat{y}|x)) + \alpha | \underset{\hat{y}}{\arg \max} \ p(\hat{y} | x) - y |
$$</p><p>This loss can be easily implemented in pytorch:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ce <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>cross_entropy(input, target, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;none&#34;</span>)
</span></span><span style=display:flex><span>reg <span style=color:#f92672>=</span> target <span style=color:#f92672>-</span> input<span style=color:#f92672>.</span>argmax(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>reg <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>abs(reg)
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> ce <span style=color:#f92672>+</span> alpha <span style=color:#f92672>*</span> reg
</span></span></code></pre></div><p>(<em>Torch example of the proposed ordinal cross entropy. For more details check <a href=https://github.com/candeiasalexandre/ordinal-classification-loss/blob/d0a0cf3d9a5e77cfa14772ea0b584e0f045e1903/ordinal_classification_loss/loss.py#L4>here</a>.</em>)</p><p>Looking at the output of the new loss for the example above we se that we have the desired behaviour:</p><p>$$
y_1=1, l_1=5.8194 \\
y_2=1, l_2=1.4194 \\
y_3=1, l_3=2.8194
$$</p><p>making the loss higher when the prediction is more distant from the label.</p><p>At a first glance, one might think that with this loss we accomplished our goal and we can now put our model training and have great results…. However, we need to take a closer look of what happens with the gradients of this new loss.</p><p>The first part of the loss is fine since is the standard cross entropy, however the second part might cause problems. Think a bit of how one would calculate the gradient of $\arg \max$ with respect to the model parameters.</p><p>In fact, the $\arg \max$ might not even be differentiable making it difficult to calculate gradients. For an in depth mathematical analysis regarding $\arg \max$ please check <a href=https://arxiv.org/pdf/2301.07473.pdf>this</a> paper from the (awesome) <a href=https://sardine-lab.github.io/>SARDINE</a> NLP group in Lisbon.</p><p>To check the gradients we do a simple test with torch using the same data as in the previous examples.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>logits_ce <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(logits<span style=color:#f92672>.</span>numpy(), requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>loss_ce <span style=color:#f92672>=</span> cross_entropy(logits_ce, labels, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;mean&#34;</span>)
</span></span><span style=display:flex><span>loss_ce<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>grad_ce <span style=color:#f92672>=</span> logits_ce<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits_oce <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(logits<span style=color:#f92672>.</span>numpy(), requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>loss_ordinal_ce <span style=color:#f92672>=</span> non_diff_ordinal_cross_entropy(
</span></span><span style=display:flex><span>    logits_oce, labels, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;mean&#34;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>loss_ordinal_ce<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>grad_oce <span style=color:#f92672>=</span> logits_oce<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><p>(<em>Torch example to compute gradients, for more detail check <a href=https://github.com/candeiasalexandre/ordinal-classification-loss/blob/d0a0cf3d9a5e77cfa14772ea0b584e0f045e1903/tests/test_loss.py#L49>github</a>.</em>)</p><p>We arrive at the conclusion <strong>that the gradients of the two losses</strong> (computed according to torch automatic differentiation) <strong>are in fact the same</strong>:</p><p><img loading=lazy src=/posts/img/an-ordinal-classification-loss-part_1/gradients.png alt="Gradient values"></p><p>This poses a challenge since even though the loss is capturing what we want, the gradients are not and that will make the updates on the model parameters unaware of the ordering structure.</p><p>In the 2nd part of this blogpost, we will see how to solve this issue by introducing another type of regularisation that doesn’t have the non differential issue but still captures the ordering relationships of our classes.</p><p><strong>You can find the code used in this post <a href=https://github.com/candeiasalexandre/ordinal-classification-loss/tree/main>here</a>.</strong></p><p>Thanks for bearing with me and my awful math notation. See you soon :)</p><p><em>Acknowledgements</em></p><p>Most of the ideias present in this blogpost are a result of discussions with my work colleague <a href=https://www.linkedin.com/in/ivo-silva-744021109/>Ivo Silva</a>.</p></article></main></div><footer><div class=req-js><button class=outline-dashed title="Change to light/dark mode."><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true"><use xlink:href="/img/bundle.min.051eb5a3ff25c1d041352b9054013bb6.svg#adjust"/></svg></button><input class=outline-dashed type=color list=presets value=#1dbc91 title="Change accent color." aria-label="Change accent color."><datalist id=presets><option value=#1dbc91></datalist></div></footer><link rel=stylesheet href=https://candeiasalexandre.github.io/libs/katex@0.16.0/dist/katex.min.6950e59dbd8dfddd111390d85888bb5f9dc2e9c334da7ac1c3bacc92a695610d.css integrity="sha256-aVDlnb2N/d0RE5DYWIi7X53C6cM02nrBw7rMkqaVYQ0=" crossorigin=anonymous><script defer src=https://candeiasalexandre.github.io/libs/katex@0.16.0/dist/katex.min.e7c837339f838404f20674bf6c066a479026575ac8314ba5f2e35156e4591226.js integrity="sha256-58g3M5+DhATyBnS/bAZqR5AmV1rIMUul8uNRVuRZEiY=" crossorigin=anonymous></script>
<script defer src=https://candeiasalexandre.github.io/libs/katex@0.16.0/dist/contrib/mhchem.min.b693877566e4d7179120e1ff9ee007af91fdda764fed15f47a0f384433544860.js integrity="sha256-tpOHdWbk1xeRIOH/nuAHr5H92nZP7RX0eg84RDNUSGA=" crossorigin=anonymous></script>
<script defer src=https://candeiasalexandre.github.io/libs/katex@0.16.0/dist/contrib/copy-tex.min.3bc3978c11fdd1ccebd83687e41d1a35adde9998ca6d2a630eff53fb9ea73040.js integrity="sha256-O8OXjBH90czr2DaH5B0aNa3emZjKbSpjDv9T+56nMEA=" crossorigin=anonymous></script>
<script defer src=https://candeiasalexandre.github.io/js/katex-custom-render.min.e642deed57e2029089e43c36d322ad7e365664d88ea56f9be51631877e2c67d0.js integrity="sha256-5kLe7VfiApCJ5Dw20yKtfjZWZNiOpW+b5RYxh34sZ9A=" crossorigin=anonymous></script></body></html>